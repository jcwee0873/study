{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import torch\n",
    "from torch import MobileOptimizerType\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from seq2seq.model.data_loader import DataLoader\n",
    "import seq2seq.model.data_loader as data_loader\n",
    "from seq2seq.model.seq2seq import Seq2Seq\n",
    "\n",
    "from seq2seq.model.trainer import SingleTrainer\n",
    "from seq2seq.model.trainer import MaximumLikelihoodEstimationEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args :\n",
    "    def __init__(self):\n",
    "        self.model_fn = './model.pth'\n",
    "        self.gpu_id = 0\n",
    "        self.n_epochs = 5\n",
    "        self.lr_scheduler = None\n",
    "        self.verbose = 2\n",
    "        self.init_epoch = 0\n",
    "        self.iteration_per_update = 1\n",
    "        self.max_grad_norm = 5.\n",
    "        self.use_noam_decay = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'gpu_id': 0,\n",
      "    'init_epoch': 0,\n",
      "    'iteration_per_update': 1,\n",
      "    'lr_scheduler': None,\n",
      "    'max_grad_norm': 5.0,\n",
      "    'model_fn': './model.pth',\n",
      "    'n_epochs': 5,\n",
      "    'use_noam_decay': False,\n",
      "    'verbose': 2}\n"
     ]
    }
   ],
   "source": [
    "config = Args()\n",
    "\n",
    "def print_config(config):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(vars(config))\n",
    "\n",
    "print_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108264 120302\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(\n",
    "    './kor_eng_translation/train copy.tsv',\n",
    "    './kor_eng_translation/valid copy.tsv',\n",
    "    batch_size=20,\n",
    ")\n",
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (emb_src): Embedding(108264, 512)\n",
      "  (emb_dec): Embedding(120302, 512)\n",
      "  (encoder): Encoder(\n",
      "    (rnn): LSTM(512, 256, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn): LSTM(1024, 512, num_layers=4, batch_first=True, dropout=0.2)\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (linear): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (concat): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (tanh): Tanh()\n",
      "  (generator): Generator(\n",
      "    (output): Linear(in_features=512, out_features=120302, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n",
      "NLLLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def get_crit(output_size, pad_index):\n",
    "    loss_weight = torch.ones(output_size)\n",
    "    loss_weight[pad_index] = 0.\n",
    "\n",
    "    crit = nn.NLLLoss(weight=loss_weight, reduction='sum')\n",
    "    return crit\n",
    "\n",
    "model = Seq2Seq(input_size, 512, 512, output_size)\n",
    "crit = get_crit(output_size, data_loader.PAD)\n",
    "\n",
    "if config.gpu_id >= 0:\n",
    "    model.cuda(config.gpu_id)\n",
    "    crit.cuda(config.gpu_id)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "print(model)\n",
    "print(crit)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b358c093bc54ef8a50cba98cb64924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                         | 1/18140 [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.99 GiB total capacity; 12.74 GiB already allocated; 0 bytes free; 13.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.99 GiB total capacity; 12.74 GiB already allocated; 0 bytes free; 13.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.99 GiB total capacity; 12.74 GiB already allocated; 0 bytes free; 13.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jcwee/src/nlp_fastcampus/NLG/05. seq2seq_code.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m SingleTrainer(MaximumLikelihoodEstimationEngine, config)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m     crit,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m     optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mloader\u001b[39m.\u001b[39;49mtrain_iter,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m     valid_loader\u001b[39m=\u001b[39;49mloader\u001b[39m.\u001b[39;49mvalid_iter,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m     src_vocab\u001b[39m=\u001b[39;49mloader\u001b[39m.\u001b[39;49msrc\u001b[39m.\u001b[39;49mvocab,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m     tgt_vocab\u001b[39m=\u001b[39;49mloader\u001b[39m.\u001b[39;49mtgt\u001b[39m.\u001b[39;49mvocab,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mn_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=10'>11</a>\u001b[0m     lr_scheduler\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jcwee/src/nlp_fastcampus/NLG/05.%20seq2seq_code.ipynb#ch0000005vscode-remote?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py:305\u001b[0m, in \u001b[0;36mSingleTrainer.train\u001b[0;34m(self, model, crit, optimizer, train_loader, valid_loader, src_vocab, tgt_vocab, n_epochs, lr_scheduler)\u001b[0m\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=292'>293</a>\u001b[0m validation_engine\u001b[39m.\u001b[39madd_event_handler(\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=293'>294</a>\u001b[0m     Events\u001b[39m.\u001b[39mEPOCH_COMPLETED, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_engine_class\u001b[39m.\u001b[39mcheck_best\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=294'>295</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=295'>296</a>\u001b[0m validation_engine\u001b[39m.\u001b[39madd_event_handler(\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=296'>297</a>\u001b[0m     Events\u001b[39m.\u001b[39mEPOCH_COMPLETED,\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=297'>298</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_engine_class\u001b[39m.\u001b[39msave_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=301'>302</a>\u001b[0m     tgt_vocab,\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=302'>303</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=304'>305</a>\u001b[0m train_engine\u001b[39m.\u001b[39;49mrun(train_loader, max_epochs\u001b[39m=\u001b[39;49mn_epochs)\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=306'>307</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:704\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=700'>701</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mepoch_length should be provided if data is None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=702'>703</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mdataloader \u001b[39m=\u001b[39m data\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=703'>704</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_run()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:783\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=780'>781</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=781'>782</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEngine run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=782'>783</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=784'>785</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=785'>786</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:466\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=463'>464</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=464'>465</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=465'>466</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:753\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=749'>750</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=750'>751</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_engine()\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=752'>753</a>\u001b[0m time_taken \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once_on_dataset()\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=753'>754</a>\u001b[0m \u001b[39m# time is available for handlers but must be update after fire\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=754'>755</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mtimes[Events\u001b[39m.\u001b[39mEPOCH_COMPLETED\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m time_taken\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:854\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=851'>852</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=852'>853</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent run is terminating due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=853'>854</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_exception(e)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=855'>856</a>\u001b[0m \u001b[39mreturn\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:466\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=463'>464</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=464'>465</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=465'>466</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py:840\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=837'>838</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39miteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=838'>839</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mITERATION_STARTED)\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=839'>840</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_function(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mbatch)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=840'>841</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fire_event(Events\u001b[39m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/ignite/engine/engine.py?line=842'>843</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_terminate \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_terminate_single_epoch:\n",
      "File \u001b[0;32m~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py:76\u001b[0m, in \u001b[0;36mMaximumLikelihoodEstimationEngine.train\u001b[0;34m(engine, mini_batch)\u001b[0m\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=72'>73</a>\u001b[0m x, y \u001b[39m=\u001b[39m mini_batch\u001b[39m.\u001b[39msrc, mini_batch\u001b[39m.\u001b[39mtgt[\u001b[39m0\u001b[39m][:, \u001b[39m1\u001b[39m:]\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=74'>75</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[0;32m---> <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=75'>76</a>\u001b[0m     y_hat \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mmodel(x, mini_batch\u001b[39m.\u001b[39;49mtgt[\u001b[39m0\u001b[39;49m][:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=76'>77</a>\u001b[0m     \u001b[39m# |y_hat| = (bs, length, output_size)\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=77'>78</a>\u001b[0m     loss \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mcrit(\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=78'>79</a>\u001b[0m         y_hat\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, y_hat\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)),\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=79'>80</a>\u001b[0m         \u001b[39m# |y_hat.view| = (bs, length)\u001b[39;00m\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=80'>81</a>\u001b[0m         y\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/trainer.py?line=81'>82</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py:283\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=279'>280</a>\u001b[0m emb_t \u001b[39m=\u001b[39m emb_tgt[:, t, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=280'>281</a>\u001b[0m \u001b[39m# |emb_t| = (bs, 1(unsqueeze), word_Vec_size)\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=282'>283</a>\u001b[0m decoder_output, decoder_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(emb_t,\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=283'>284</a>\u001b[0m                                               h_t_tilde,\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=284'>285</a>\u001b[0m                                               decoder_hidden\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=285'>286</a>\u001b[0m                                              )\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=286'>287</a>\u001b[0m \u001b[39m# |decoder_output| = (bs, 1, hs)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=287'>288</a>\u001b[0m \u001b[39m# |decoder_hidden| = (n_layers, bs, hs)\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=288'>289</a>\u001b[0m context_vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(h_src, decoder_output, mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py:116\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, emb_t, h_t_1_tilde, h_t_1)\u001b[0m\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=110'>111</a>\u001b[0m     h_t_1_tilde \u001b[39m=\u001b[39m emb_t\u001b[39m.\u001b[39mnew(batch_size, \u001b[39m1\u001b[39m, hidden_size)\u001b[39m.\u001b[39mzero_()\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=113'>114</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([emb_t, h_t_1_tilde], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=115'>116</a>\u001b[0m y, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h_t_1)\n\u001b[1;32m    <a href='file:///~/src/nlp_fastcampus/NLG/seq2seq/model/seq2seq.py?line=117'>118</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y, h\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py:691\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=688'>689</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=689'>690</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=690'>691</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=691'>692</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=692'>693</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=693'>694</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/torch110/lib/python3.9/site-packages/torch/nn/modules/rnn.py?line=694'>695</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.99 GiB total capacity; 12.74 GiB already allocated; 0 bytes free; 13.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = SingleTrainer(MaximumLikelihoodEstimationEngine, config)\n",
    "trainer.train(\n",
    "    model,\n",
    "    crit,\n",
    "    optimizer,\n",
    "    train_loader=loader.train_iter,\n",
    "    valid_loader=loader.valid_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    n_epochs=config.n_epochs,\n",
    "    lr_scheduler=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "452df0821433779ea0d62ff49f1a7cc03808e7ef127810619f5ecf0887789e09"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
