{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34f680d7",
   "metadata": {},
   "source": [
    "## BeamSearch\n",
    "- Generation == Search Problem\n",
    "- Greedy Search: 지금의 최선이 나중에는 나쁜 선택이 될 수 있음\n",
    "- Beam Search: top-k를 tracking하여 greedy search를 조금 더 안전하게 수행\n",
    "- Beam Search를 병렬처리하면 속도와 성능 모두 만족스러울 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f3679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd4bac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab66b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104b826",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4206e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=word_vec_size,\n",
    "            hidden_size=int(hidden_size / 2),\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, emb):\n",
    "        if isinstance(emb, tuple):\n",
    "            x, lengths = emb\n",
    "            x = pack(x, lengths.tolist(), batch_first=True)\n",
    "        \n",
    "        else:\n",
    "            x = emb\n",
    "            \n",
    "        y, h = self.rnn(x)\n",
    "        \n",
    "        if isinstance(emb, tuple):\n",
    "            y, _ = unpack(y, batch_first=True)\n",
    "        \n",
    "        return y, h    \n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_hidden, mask=None):\n",
    "        # |decoder_hidden| = (bs, 1, hidden_size)\n",
    "        # |encoder_hidden| = (bs, n, hidden_size)\n",
    "        \n",
    "        query = self.linear(decoder_hidden)\n",
    "        # |query| = (bs, 1, hidden_size)\n",
    "        \n",
    "        weight = torch.bmm(query, encoder_hidden.transpose(1, 2))\n",
    "        # |weight| = (bs, 1, hidden_size) dot (bs, hidden_size, n)\n",
    "        #          = (bs, 1, n)\n",
    "        \n",
    "        if mask is not None:\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
    "            \n",
    "        weight = self.softmax(weight)\n",
    "        \n",
    "        value = torch.bmm(weight, encoder_hidden)\n",
    "        # |value| = (bs, 1, n) dot (bs, n, hidden_size)\n",
    "        #         = (bs, 1, hidden_size)\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=word_vec_size + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, emb_t, h_prev_tilde, h_prev):\n",
    "        batch_size = emb_t.size(0)\n",
    "        hidden_size = h_prev.size(-1)\n",
    "        \n",
    "        if h_prev_tilde is None:\n",
    "            h_prev_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n",
    "        \n",
    "        x = torch.cat([emb_t, h_prev_tilde], dim=-1)\n",
    "        \n",
    "        y, h = self.rnn(x, h_prev)\n",
    "        \n",
    "        return y, h\n",
    "    \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.output(x)\n",
    "        y = self.softmax(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 word_vec_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 n_layers=4,\n",
    "                 dropout_p=.2\n",
    "                ):\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder_emb = nn.Embedding(input_size, word_vec_size)\n",
    "        self.decoder_emb = nn.Embedding(output_size, word_vec_size)\n",
    "        \n",
    "        self.encoder = Encoder(word_vec_size, hidden_size, n_layers=n_layers, dropout_p=dropout_p)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = Decoder(word_vec_size, hidden_size, n_layers=n_layers, dropout_p=dropout_p)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.generator = Generator(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def merge_z(self, z):\n",
    "        # |z| = (n_layers * 2, bs, hidden_size / 2)\n",
    "        batch_size = z.size(1)\n",
    "        \n",
    "        z = z.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                -1,\n",
    "                                                self.hidden_size).transpose(0, 1).contiguous()\n",
    "        # |.transpose(0, 1| = (bs, n_layers * 2, hidden_size / 2)\n",
    "        # |.view| = (bs, n_layers, hidden_size)\n",
    "        # |.transpose(0, 1)| = (n_layers, bs, hidden_size)\n",
    "        # |z| = (n_layers, bs, hidden_size)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    \n",
    "    def generate_mask(self, x, length):\n",
    "        mask = []\n",
    "\n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # If the length is shorter than maximum length among samples, \n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                    x.new_ones(1, (max_length - l))\n",
    "                                    ], dim=-1)]\n",
    "            else:\n",
    "                # If the length of the sample equals to maximum length among samples, \n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "        mask = torch.cat(mask, dim=0).bool()\n",
    "\n",
    "        return mask\n",
    "    \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # |src| = (bs, n, |V|)\n",
    "        # |tgt| = (bs, m, |V|)\n",
    "        \n",
    "        batch_size = tgt.size(0)\n",
    "        \n",
    "        mask = None\n",
    "        x_length = None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "        \n",
    "        else :\n",
    "            x = src\n",
    "        \n",
    "        encoder_emb_vec = self.encoder_emb(x)\n",
    "        # |encoder_emb_vec| = (bs, n, word_vec_size)\n",
    "        \n",
    "        encoder_hidden, z = self.encoder((encoder_emb_vec, x_length))\n",
    "        # |encoder_hidden| = (bs, n, hidden_size)\n",
    "        # |z| = (n_layers * 2, bs, hidden_size / 2)\n",
    "        \n",
    "        z = self.merge_z(z)\n",
    "        # |z| = (n_layers, bs, hidden_size)\n",
    "        \n",
    "        decoder_emb_vec = self.decoder_emb(tgt)\n",
    "        # |decoder_emb_vec| = (bs, m, word_vec_size)\n",
    "        \n",
    "        h_tilde = []\n",
    "        \n",
    "        h_t_tilde = None\n",
    "        decoder_hidden = z\n",
    "        \n",
    "        for t in range(tgt.size(1)) :\n",
    "            \n",
    "            emb_t = decoder_emb_vec[:, t, :].unsqueeze(1)\n",
    "            # |emb_t| = (bs, 1, word_vec_size)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, h_t_tilde, decoder_hidden)\n",
    "            # |decoder_output| = (bs, 1, hidden_size)\n",
    "            # |decoder_hidden| = (n_layers, bs, hidden_size)\n",
    "            \n",
    "            context_vector = self.attention(decoder_output, encoder_hidden, mask)\n",
    "            # |context_vector| = (bs, 1, hidden_size)\n",
    "            \n",
    "            h_t_tilde = torch.cat([decoder_output, context_vector], dim=-1)\n",
    "            # |h_t_tilde| = (bs, 1, hidden_size * 2)\n",
    "            \n",
    "            h_t_tilde = self.concat(h_t_tilde)\n",
    "            # |h_t_tilde| = (bs, 1, hidden_size)\n",
    "            \n",
    "            h_t_tilde = self.tanh(h_t_tilde)\n",
    "            \n",
    "            h_tilde += [h_t_tilde]\n",
    "            \n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "        # |h_tilde| = (bs, m, hidden_size)\n",
    "        \n",
    "        y_hat = self.generator(h_tilde)\n",
    "        # |y_hat| = (bs, m, output_size)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def search(self, src, is_greedy=True, max_length=255):\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "            \n",
    "        else:\n",
    "            x, x_length = src, None\n",
    "            mask = None\n",
    "            \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        encoder_emb_vec = self.encoder_emb(x)\n",
    "        encoder_hidden, z = self.encoder(encoder_emb_vec)\n",
    "        \n",
    "        z = self.merge_z(z)\n",
    "        \n",
    "        y = x.new(batch_size, 1).zero_() + 2 # index of <BOS>\n",
    "        \n",
    "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
    "        \n",
    "        h_t_tilde, y_hats, indice = None, [], []\n",
    "        decoder_hidden = z\n",
    "        \n",
    "        \n",
    "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
    "            emb_t = self.decoder_emb(y)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, h_t_tilde, decoder_hidden)\n",
    "            context_vector = self.attention(decoder_output, encoder_hidden, mask)\n",
    "\n",
    "            h_t_tilde = torch.cat([decoder_output, context_vector], dim=-1)\n",
    "            h_t_tilde = self.concat(h_t_tilde)\n",
    "            h_t_tilde = self.tanh(h_t_tilde)\n",
    "            \n",
    "            y_hat = self.generator(h_t_tilde)\n",
    "            \n",
    "            y_hats += [y_hat]\n",
    "            \n",
    "            if is_greedy:\n",
    "                y = y_hat.argmax(dim=-1)\n",
    "                \n",
    "            else:\n",
    "                # take random sampling\n",
    "                y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1)\n",
    "                \n",
    "            # 이번 step에 EOS가 없을 경우 PAD\n",
    "            y = y.masked_fill_(~is_decoding, 0) # index of <PAD>\n",
    "            # 있을 경우 EOS\n",
    "            is_decoding = is_decoding * torch.ne(y, 3) # index of <EOS>\n",
    "            \n",
    "            indice += [y]\n",
    "            \n",
    "        y_hats = torch.cat(y_hats, dim=1)\n",
    "        indice = torch.cat(indice, dim=1)\n",
    "        \n",
    "        return y_hats, indice\n",
    "    \n",
    "    \n",
    "    def batch_beam_search(\n",
    "        self,\n",
    "        src,\n",
    "        beam_size=5,\n",
    "        max_length=255,\n",
    "        n_best=1,\n",
    "        length_penalty=.2\n",
    "    ):\n",
    "        mask, x_length = None, None\n",
    "        \n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "            \n",
    "        else:\n",
    "            x = src\n",
    "            \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        encoder_emb_vec = self.encoder_emb(x)\n",
    "        encoder_hidden, z = self.encoder((encoder_emb_vec, x_length))\n",
    "        z = self.merge_z(z)\n",
    "        \n",
    "        \n",
    "        boards = [SingleBeamSearchBoard(\n",
    "            encoder_hidden.device, \n",
    "            {\n",
    "                'hidden_state': {\n",
    "                    'init_status': z[:, i, :].unsqueeze(1),\n",
    "                    'batch_dim_index': 1,\n",
    "                },\n",
    "                'h_prev_tilde': {\n",
    "                    'init_status': None,\n",
    "                    'batch_dim_index': 0\n",
    "                }\n",
    "            },\n",
    "            beam_size=beam_size,\n",
    "            max_length=max_length,\n",
    "        ) for i in range(batch_size)]\n",
    "        done_cnt = [board.is_done() for board in boards]\n",
    "        \n",
    "        length = 0\n",
    "        \n",
    "        while sum(done_cnt) < batch_size and length <= max_length:\n",
    "            \n",
    "            fab_input, fab_hidden, fab_h_t_tilde = [], [], []\n",
    "            fab_encoder_hidden, fab_mask = [], []\n",
    "            \n",
    "            for i, board in enumerate(boards):\n",
    "                if not board.is_done():\n",
    "                    y_hat_i, prev_status = board.get_batch()\n",
    "                    hidden_i = prev_status['hidden_state']\n",
    "                    h_t_tilde_i = prev_status['h_prev_tilde']\n",
    "                    \n",
    "                    fab_input += [y_hat_i]\n",
    "                    fab_hidden += [hidden_i]\n",
    "                    fab_encoder_hidden += [encoder_hidden[i, :, :]] * beam_size\n",
    "                    fab_mask += [mask[i, :]] * beam_size\n",
    "                    if h_t_tilde_i is not None:\n",
    "                        fab_h_t_tilde += [h_t_tilde_i]\n",
    "                    else:\n",
    "                        fab_h_t_tilde = None\n",
    "                        \n",
    "            # 가짜 미니배치\n",
    "            fab_input = torch.cat(fab_input, dim=0)\n",
    "            fab_hidden = torch.cat(fab_hidden, dim=1)\n",
    "            fab_encoder_hidden = torch.stack(fab_encoder_hidden)\n",
    "            fab_mask = torch.stack(fab_mask)\n",
    "            if fab_h_t_tilde is not None:\n",
    "                fab_h_t_tilde = torch.cat(fab_h_t_tilde, dim=0)\n",
    "                \n",
    "            decoder_emb_vec = self.decoder_emb(fab_input)\n",
    "            \n",
    "            fab_decoder_output, fab_hidden = self.decoder(decoder_emb_vec,\n",
    "                                                          fab_h_t_tilde,\n",
    "                                                          fab_hidden\n",
    "                                                         )\n",
    "            \n",
    "            context_vector = self.attn(fab_encoder_hidden, fab_decoder_output, fab_mask)\n",
    "            fab_h_t_tilde = torch.cat([fab_decoder_output, context_vector], dim=-1)\n",
    "            fab_h_t_tilde = self.concat(fab_h_t_tilde)\n",
    "            fab_h_t_tilde = self.tanh(fab_h_t_tilde)\n",
    "            \n",
    "            y_hat = self.generator(fab_h_t_tilde)\n",
    "            \n",
    "            cnt = 0\n",
    "            for board in boards:\n",
    "                if not board.is_done():\n",
    "                    begin = cnt * beam_size\n",
    "                    end = begin + beam_size\n",
    "                    \n",
    "                    board.collect_result(\n",
    "                        y_hat[begin:end],\n",
    "                        {\n",
    "                            'hidden_state': fab_hidden[:, begin:end, :],\n",
    "                            'h_prev_tilde': fab_h_t_tilde[begin:end]\n",
    "                        }\n",
    "                    )\n",
    "                    cnt += 1\n",
    "            done_cnt = [board.is_done() for bard in boards]\n",
    "            length += 1\n",
    "            \n",
    "        batch_sentences, batch_probs = [], []\n",
    "        \n",
    "        for i, board in enumerate(boards):\n",
    "            sentences, probs = board.get_n_best(n_best, length_penalty=length_penalty)\n",
    "            \n",
    "            batch_sentences += [sentences]\n",
    "            batch_probs += [probs]\n",
    "            \n",
    "        return batch_sentences, batch_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d67be",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb1e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "LENGTH_PENALTY = .2\n",
    "MIN_LENGTH = 5\n",
    "\n",
    "class SingleBeamSearchBoard():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        prev_status_config,\n",
    "        beam_sizem=5,\n",
    "        max_length=255,\n",
    "    ):\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.device = device\n",
    "        # BOS 미리 지정\n",
    "        self.word_indice = [torch.LongTensor(beam_size).zero_().to(self.device) + 2]\n",
    "        # 이전의 beam 위치, -1에서 위치로 바뀔 것\n",
    "        self.beam_indice = [torch.LongTensor(beam_size).zero_().to(self.device) - 1]\n",
    "        # 각 beam에 대한 누적확률, BOS가 들어갔을 때 나머지 확률을 무시하기 위해 -inf 처리\n",
    "        self.cumulative_probs = [torch.FloatTensor([.0] + [-float('inf')] * (beam_size - 1)).to(self.device)]\n",
    "        self.masks = [torch.BoolTensor(beam_size).zero_().to(self.device)]\n",
    "        \n",
    "        self.prev_status = {}\n",
    "        self.batch_dims = {}\n",
    "        \n",
    "        # hidden_state, h_prev_tilde 초기화\n",
    "        for prev_status_name, each_config in prev_status_config.items():\n",
    "            init_status = each_config['init_status']\n",
    "            # if hidden_state, |init_status| = (n_layers, 1, hidden_size)\n",
    "            batch_dim_index = each_config['batch_dim_index']\n",
    "            \n",
    "            if init_status is not None:\n",
    "                self.prev_status[prev_status_name] = torch.cat([init_satus] * beam_size,\n",
    "                                                               dim=batch_dim_index)\n",
    "                # if hidden_state, |prev_status[prev_status_name]| = (n_layers, beam_size, hidden_size)\n",
    "            else:\n",
    "                self.prev_status[prev_status_name] = None\n",
    "            self.batch_dims[prev_status_name] = batch_dim_index\n",
    "            \n",
    "        self.current_time_step = 0\n",
    "        self.done_cnt = 0\n",
    "        \n",
    "    \n",
    "    def get_length_penalty(\n",
    "        self,\n",
    "        length,\n",
    "        alpha=LENGTH_PENALTY,\n",
    "        min_lengt=MIN_LENGTH,\n",
    "    ):\n",
    "        p = ((min_length + 1) / (min_length + length)) ** alpha\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    \n",
    "    def is_done(self):\n",
    "        return self.done_cnt >= self.beam_size\n",
    "    \n",
    "    \n",
    "    def get_batch(self):\n",
    "        y_hat = self.word_indice[-1].unsqueeze(-1)\n",
    "        # |y_hat| = (beam_size, 1)\n",
    "#         prev_status = [v for k, v in self.prev_status.items()]\n",
    "        \n",
    "        return y_hat, self.prev_status\n",
    "    \n",
    "    def collect_result(self, y_hat, prev_status):\n",
    "        output_size = y_hat.size(-1)\n",
    "        \n",
    "        self.current_time_step += 1\n",
    "        \n",
    "        # 이미 끝난 beam은 -inf 통해 안보겠다.\n",
    "        cumulative_prob = self.cumulative_probs[-1].masked_fill_(self.masks[-1], -float('inf'))\n",
    "        cumulative_prob = y_hat + cumulative_prob.view(-1, 1, 1).expand(self.beam_size, 1, output_size)\n",
    "        \n",
    "        top_log_prob, top_indice = torch.topk(\n",
    "            cumulative_prob.view(-1),\n",
    "            self.beam_size,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # Beam 내에서의 순서\n",
    "        self.word_indice += [top_indice.fmod(output_size)]\n",
    "        # Beam 순서\n",
    "        self.beam_indice += [top_indice.div(float(output_size)).long()]\n",
    "        \n",
    "        self.cumulative_probs += [top_log_prob]\n",
    "        self.masks += [torch.eq(self.word_indice[-1], 3)]\n",
    "        self.done_cnt += self.masks[-1].float().sum()\n",
    "        \n",
    "        \n",
    "        for prev_status_name, prev_status in prev_status.items():\n",
    "            self.prev_status[prev_status_name] = torch.index_select(\n",
    "                prev_status,\n",
    "                dim=self.batch_dims[prev_status_name],\n",
    "                index=self.beam_indice[-1]\n",
    "            ).contiguous()\n",
    "            \n",
    "            \n",
    "    def get_n_best(self, n=1, length_penalty=.2):\n",
    "        sentences, probs, founds = [], [], []\n",
    "        \n",
    "        for t in range(len(self.word_indice)):\n",
    "            for b in range(self.beam_size):\n",
    "                if self.masks[t][b] == 1: # eos 찾기\n",
    "                    probs += [self.cumulative_probs[t][b] * self.get_length_penalty(t, alpha=length_penalty)]\n",
    "                    founds += [(t, b)]\n",
    "                    \n",
    "        for b in range(self.beam_size):\n",
    "            if self.cumulative_probs[-1][b] != -float('inf'):\n",
    "                if not (len(self.cumulative_probs) - 1, b) in founds:\n",
    "                    probs += [self.cumulative_probs[-1][b] * self.get_length_penalty(len(self.cumulative_probs), alpha=length_penalty)]\n",
    "                    founds += [(t, b)]\n",
    "                    \n",
    "                    \n",
    "        sorted_founds_with_probs = sorted(\n",
    "            zip(founds, probs),\n",
    "            key=itemgetter(1),\n",
    "            reverse=True\n",
    "        )[:n]\n",
    "        probs = []\n",
    "        \n",
    "        for (end_index, b), prob in sorted_founds_woth_probs:\n",
    "            sentence = []\n",
    "            \n",
    "            for t in range(end_index, 0, -1):\n",
    "                sentence = [self.word_indice[t][b]] + sentence\n",
    "                b = self.beam_indice[t][b]\n",
    "                \n",
    "            sentences += [sentence]\n",
    "            probs += [prob]\n",
    "            \n",
    "        return sentences, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedce20b",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d475860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, batch_size=64, max_length=70, shuffle=True, train=True):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.SRC = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            preprocessing=lambda x : x if len(x) < max_length else x[:max_length],\n",
    "            include_lengths=True,\n",
    "        )\n",
    "        self.TGT = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            preprocessing=lambda x : x if len(x) < max_length else x[:max_length],\n",
    "            init_token='<BOS>',\n",
    "            eos_token='<EOS>'\n",
    "        )\n",
    "        \n",
    "        if train :\n",
    "            train, valid = data.TabularDataset.splits(\n",
    "                path='./kor_eng_translation/',\n",
    "                train='train.tsv',\n",
    "                validation='valid.tsv',\n",
    "                format='tsv',\n",
    "                fields=[('src',self.SRC), ('tgt', self.TGT)]\n",
    "            )\n",
    "\n",
    "            self.train_loader = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size,\n",
    "                device='cuda:0',\n",
    "                shuffle=shuffle,\n",
    "                sort_key = lambda x : len(x.tgt) + (80 * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "            self.valid_loader = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size,\n",
    "                device='cuda:0',\n",
    "                sort_key = lambda x : len(x.tgt) + (80 * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "            \n",
    "            self.SRC.build_vocab(train, max_size=30000, min_freq=5)\n",
    "            self.TGT.build_vocab(train, max_size=30000, min_freq=5)\n",
    "            \n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.SRC.vocab = src_vocab\n",
    "        self.TGT.vocab = tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f8cee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
