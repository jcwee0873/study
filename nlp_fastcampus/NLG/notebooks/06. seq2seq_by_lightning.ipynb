{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b05c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb373fb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30ecb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(pl.LightningModule):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=word_vec_size,\n",
    "            hidden_size=int(hidden_size / 2),\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y, h = self.rnn(x)\n",
    "        \n",
    "        return y, h    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a995d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(pl.LightningModule):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_hidden, mask=None):\n",
    "        # |decoder_hidden| = (bs, 1, hidden_size)\n",
    "        # |encoder_hidden| = (bs, n, hidden_size)\n",
    "        \n",
    "        query = self.linear(decoder_hidden)\n",
    "        # |query| = (bs, 1, hidden_size)\n",
    "        \n",
    "        weight = torch.bmm(query, encoder_hidden.transpose(1, 2))\n",
    "        # |weight| = (bs, 1, hidden_size) dot (bs, hidden_size, n)\n",
    "        #          = (bs, 1, n)\n",
    "        \n",
    "        if mask is not None:\n",
    "            weight.masked_fill_(mask.unsqueeze(-1), -np.inf)\n",
    "            \n",
    "        weight = self.softmax(weight)\n",
    "        \n",
    "        value = torch.bmm(weight, encoder_hidden)\n",
    "        # |value| = (bs, 1, n) dot (bs, n, hidden_size)\n",
    "        #         = (bs, 1, hidden_size)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3506f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(pl.LightningModule):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=word_vec_size + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, emb_t, h_prev_tilde, h_prev):\n",
    "#         print('|emb_t|: ', emb_t.shape)\n",
    "#         print('|h_prev_tilde|: ', h_prev_tilde.shape if h_prev_tilde is not None else 'None')\n",
    "#         print('|h_prev|: ', h_prev.shape)\n",
    "        if h_prev_tilde is None:\n",
    "            batch_size = emb_t.size(0)\n",
    "            hidden_size = h_prev.size(-1)\n",
    "            \n",
    "            h_prev_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n",
    "#             print('New |h_tilde|:', h_prev_tilde.shape)\n",
    "        \n",
    "        x = torch.cat([emb_t, h_prev_tilde], dim=-1)\n",
    "#         print('|x|:', x.shape)\n",
    "        \n",
    "        y, h = self.rnn(x, h_prev)\n",
    "        \n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c714cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(pl.LightningModule):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.output(x)\n",
    "        y = self.softmax(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465b508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 word_vec_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 n_layers=4,\n",
    "                 dropout_p=.2\n",
    "                ):\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder_emb = nn.Embedding(input_size, word_vec_size)\n",
    "        self.decoder_emb = nn.Embedding(output_size, word_vec_size)\n",
    "        \n",
    "        self.encoder = Encoder(word_vec_size, hidden_size, n_layers=n_layers, dropout_p=dropout_p)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = Decoder(word_vec_size, hidden_size, n_layers=n_layers, dropout_p=dropout_p)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.generator = Generator(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def merge_z(self, z):\n",
    "        # |z| = (n_layers * 2, bs, hidden_size / 2)\n",
    "        batch_size = z.size(1)\n",
    "        \n",
    "        z = z.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                -1,\n",
    "                                                self.hidden_size).transpose(0, 1).contiguous()\n",
    "        # |.transpose(0, 1| = (bs, n_layers * 2, hidden_size / 2)\n",
    "        # |.view| = (bs, n_layers, hidden_size)\n",
    "        # |.transpose(0, 1)| = (n_layers, bs, hidden_size)\n",
    "        # |z| = (n_layers, bs, hidden_size)\n",
    "        \n",
    "        return z\n",
    "        \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # |src| = (bs, n, |V|)\n",
    "        # |tgt| = (bs, m, |V|)\n",
    "        \n",
    "        batch_size = tgt.size(0)\n",
    "        \n",
    "        mask = None\n",
    "        \n",
    "        encoder_emb_vec = self.encoder_emb(src)\n",
    "        # |encoder_emb_vec| = (bs, n, word_vec_size)\n",
    "        \n",
    "        encoder_hidden, z = self.encoder(encoder_emb_vec)\n",
    "        # |encoder_hidden| = (bs, n, hidden_size)\n",
    "        # |z| = (n_layers * 2, bs, hidden_size / 2)\n",
    "        \n",
    "        z = self.merge_z(z)\n",
    "        # |z| = (n_layers, bs, hidden_size)\n",
    "        \n",
    "        decoder_emb_vec = self.decoder_emb(tgt)\n",
    "        # |decoder_emb_vec| = (bs, m, word_vec_size)\n",
    "        \n",
    "        h_tilde = []\n",
    "        h_t_tilde = None\n",
    "        decoder_hidden = z\n",
    "        \n",
    "        for t in range(tgt.size(1)) :\n",
    "            \n",
    "            emb_t = decoder_emb_vec[:, t, :].unsqueeze(1)\n",
    "            # |emb_t| = (bs, 1, word_vec_size)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, h_t_tilde, decoder_hidden)\n",
    "            # |decoder_output| = (bs, 1, hidden_size)\n",
    "            # |decoder_hidden| = (n_layers, bs, hidden_size)\n",
    "            \n",
    "            context_vector = self.attention(decoder_output, encoder_hidden, mask)\n",
    "            # |context_vector| = (bs, 1, hidden_size)\n",
    "            \n",
    "            h_t_tilde = torch.cat([decoder_output, context_vector], dim=-1)\n",
    "            # |h_t_tilde| = (bs, 1, hidden_size * 2)\n",
    "            \n",
    "            h_t_tilde = self.concat(h_t_tilde)\n",
    "            # |h_t_tilde| = (bs, 1, hidden_size)\n",
    "            \n",
    "            h_t_tilde = self.tanh(h_t_tilde)\n",
    "            \n",
    "            h_tilde += [h_t_tilde]\n",
    "            \n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "        # |h_tilde| = (bs, m, hidden_size)\n",
    "        \n",
    "        y_hat = self.generator(h_tilde)\n",
    "        # |y_hat| = (bs, m, output_size)\n",
    "        \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7663a6",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4492459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModule(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        super(CustomModule, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.crit = nn.NLLLoss()\n",
    "        self.optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        return self.model(src, tgt)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        logits = self(x[0], x[1])\n",
    "        loss = self.crit(logits.contiguous().view(-1, logits.size(-1)),\n",
    "                         x[1].contiguous().view(-1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        logits = self(x[0], x[1])\n",
    "        loss = self.crit(logits.contiguous().view(-1, logits.size(-1)),\n",
    "                         x[1].contiguous().view(-1))\n",
    "        metrics = {'val_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return  self.optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7ca92",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96c7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0b311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, batch_size=64):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.SRC = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            fix_length=256,\n",
    "        )\n",
    "        self.TGT = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            fix_length=256,\n",
    "            init_token='<BOS>',\n",
    "            eos_token='<EOS>'\n",
    "        )\n",
    "        \n",
    "        train, valid = data.TabularDataset.splits(\n",
    "            path='./kor_eng_translation/',\n",
    "            train='train copy.tsv',\n",
    "            validation='valid copy.tsv',\n",
    "            format='tsv',\n",
    "            fields=[('src',self.SRC), ('tgt', self.TGT)]\n",
    "        )\n",
    "        self.SRC.build_vocab(train, max_size=10000)\n",
    "        self.TGT.build_vocab(train, max_size=10000)\n",
    "        \n",
    "        self.train_loader = data.BucketIterator(\n",
    "            train,\n",
    "            batch_size,\n",
    "            device='cuda:0',\n",
    "            sort_key = lambda x : len(x.SRC)\n",
    "        )\n",
    "        self.valid_loader = data.BucketIterator(\n",
    "            valid,\n",
    "            batch_size,\n",
    "            device='cuda:0',\n",
    "            sort_key = lambda x : len(x.SRC)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "230a31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CustomDataLoader(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a4603df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 10004)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.SRC.vocab), len(dm.TGT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2afd10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(dm.SRC.vocab)\n",
    "output_size = len(dm.TGT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2559d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(input_size, 256, 256, output_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "728821bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | Seq2Seq | 10.9 M\n",
      "1 | crit  | NLLLoss | 0     \n",
      "----------------------------------\n",
      "10.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.9 M    Total params\n",
      "21.702    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcwee/anaconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:141: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n",
      "/home/jcwee/anaconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 256. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/jcwee/anaconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:92: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d957bd814fa43cbae5010cd27292905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcwee/anaconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "module = CustomModule(model)\n",
    "trainer = pl.Trainer(precision=16, max_epochs=1, gpus=1)\n",
    "trainer.fit(module, dm.train_loader, dm.valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d309979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'model': model.state_dict(),\n",
    "        'opt': module.optimizer.state_dict(),\n",
    "        'src_vocab': dm.SRC.vocab,\n",
    "        'tgt_vocab': dm.TGT.vocab,\n",
    "    }, './model.pth'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabba7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
