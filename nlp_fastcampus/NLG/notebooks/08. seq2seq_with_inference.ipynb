{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b05c75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb373fb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30ecb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=word_vec_size,\n",
    "            hidden_size=int(hidden_size / 2),\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, emb):\n",
    "        if isinstance(emb, tuple):\n",
    "            x, lengths = emb\n",
    "            x = pack(x, lengths.tolist(), batch_first=True)\n",
    "        \n",
    "        else:\n",
    "            x = emb\n",
    "            \n",
    "        y, h = self.rnn(x)\n",
    "        \n",
    "        if isinstance(emb, tuple):\n",
    "            y, _ = unpack(y, batch_first=True)\n",
    "        \n",
    "        return y, h    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a995d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_hidden, mask=None):\n",
    "        # |decoder_hidden| = (bs, 1, hidden_size)\n",
    "        # |encoder_hidden| = (bs, n, hidden_size)\n",
    "        \n",
    "        query = self.linear(decoder_hidden)\n",
    "        # |query| = (bs, 1, hidden_size)\n",
    "        \n",
    "        weight = torch.bmm(query, encoder_hidden.transpose(1, 2))\n",
    "        # |weight| = (bs, 1, hidden_size) dot (bs, hidden_size, n)\n",
    "        #          = (bs, 1, n)\n",
    "        \n",
    "        if mask is not None:\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
    "            \n",
    "        weight = self.softmax(weight)\n",
    "        \n",
    "        value = torch.bmm(weight, encoder_hidden)\n",
    "        # |value| = (bs, 1, n) dot (bs, n, hidden_size)\n",
    "        #         = (bs, 1, hidden_size)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3506f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=word_vec_size + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, emb_t, h_prev_tilde, h_prev):\n",
    "        batch_size = emb_t.size(0)\n",
    "        hidden_size = h_prev.size(-1)\n",
    "        \n",
    "        if h_prev_tilde is None:\n",
    "            h_prev_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n",
    "        \n",
    "        x = torch.cat([emb_t, h_prev_tilde], dim=-1)\n",
    "        \n",
    "        y, h = self.rnn(x, h_prev)\n",
    "        \n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c714cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.output(x)\n",
    "        y = self.softmax(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465b508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 word_vec_size,\n",
    "                 hidden_size,\n",
    "                 output_size,\n",
    "                 n_layers=4,\n",
    "                 dropout_p=.2\n",
    "                ):\n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder_emb = nn.Embedding(input_size, word_vec_size)\n",
    "        self.decoder_emb = nn.Embedding(output_size, word_vec_size)\n",
    "        \n",
    "        self.encoder = Encoder(word_vec_size, hidden_size, n_layers=n_layers, dropout_p=dropout_p)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.decoder = Decoder(word_vec_size, hidden_size, n_layers=n_layers, dropout_p=dropout_p)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.generator = Generator(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def merge_z(self, z):\n",
    "        # |z| = (n_layers * 2, bs, hidden_size / 2)\n",
    "        batch_size = z.size(1)\n",
    "        \n",
    "        z = z.transpose(0, 1).contiguous().view(batch_size,\n",
    "                                                -1,\n",
    "                                                self.hidden_size).transpose(0, 1).contiguous()\n",
    "        # |.transpose(0, 1| = (bs, n_layers * 2, hidden_size / 2)\n",
    "        # |.view| = (bs, n_layers, hidden_size)\n",
    "        # |.transpose(0, 1)| = (n_layers, bs, hidden_size)\n",
    "        # |z| = (n_layers, bs, hidden_size)\n",
    "        \n",
    "        return z\n",
    "\n",
    "    \n",
    "    def generate_mask(self, x, length):\n",
    "        mask = []\n",
    "\n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # If the length is shorter than maximum length among samples, \n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1, l).zero_(),\n",
    "                                    x.new_ones(1, (max_length - l))\n",
    "                                    ], dim=-1)]\n",
    "            else:\n",
    "                # If the length of the sample equals to maximum length among samples, \n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1, l).zero_()]\n",
    "\n",
    "        mask = torch.cat(mask, dim=0).bool()\n",
    "\n",
    "        return mask\n",
    "    \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # |src| = (bs, n, |V|)\n",
    "        # |tgt| = (bs, m, |V|)\n",
    "        \n",
    "        batch_size = tgt.size(0)\n",
    "        \n",
    "        mask = None\n",
    "        x_length = None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "        \n",
    "        else :\n",
    "            x = src\n",
    "        \n",
    "        encoder_emb_vec = self.encoder_emb(x)\n",
    "        # |encoder_emb_vec| = (bs, n, word_vec_size)\n",
    "        \n",
    "        encoder_hidden, z = self.encoder((encoder_emb_vec, x_length))\n",
    "        # |encoder_hidden| = (bs, n, hidden_size)\n",
    "        # |z| = (n_layers * 2, bs, hidden_size / 2)\n",
    "        \n",
    "        z = self.merge_z(z)\n",
    "        # |z| = (n_layers, bs, hidden_size)\n",
    "        \n",
    "        decoder_emb_vec = self.decoder_emb(tgt)\n",
    "        # |decoder_emb_vec| = (bs, m, word_vec_size)\n",
    "        \n",
    "        h_tilde = []\n",
    "        \n",
    "        h_t_tilde = None\n",
    "        decoder_hidden = z\n",
    "        \n",
    "        for t in range(tgt.size(1)) :\n",
    "            \n",
    "            emb_t = decoder_emb_vec[:, t, :].unsqueeze(1)\n",
    "            # |emb_t| = (bs, 1, word_vec_size)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, h_t_tilde, decoder_hidden)\n",
    "            # |decoder_output| = (bs, 1, hidden_size)\n",
    "            # |decoder_hidden| = (n_layers, bs, hidden_size)\n",
    "            \n",
    "            context_vector = self.attention(decoder_output, encoder_hidden, mask)\n",
    "            # |context_vector| = (bs, 1, hidden_size)\n",
    "            \n",
    "            h_t_tilde = torch.cat([decoder_output, context_vector], dim=-1)\n",
    "            # |h_t_tilde| = (bs, 1, hidden_size * 2)\n",
    "            \n",
    "            h_t_tilde = self.concat(h_t_tilde)\n",
    "            # |h_t_tilde| = (bs, 1, hidden_size)\n",
    "            \n",
    "            h_t_tilde = self.tanh(h_t_tilde)\n",
    "            \n",
    "            h_tilde += [h_t_tilde]\n",
    "            \n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "        # |h_tilde| = (bs, m, hidden_size)\n",
    "        \n",
    "        y_hat = self.generator(h_tilde)\n",
    "        # |y_hat| = (bs, m, output_size)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def search(self, src, is_greedy=True, max_length=255):\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "            \n",
    "        else:\n",
    "            x, x_length = src, None\n",
    "            mask = None\n",
    "            \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        encoder_emb_vec = self.encoder_emb(x)\n",
    "        encoder_hidden, z = self.encoder(encoder_emb_vec)\n",
    "        \n",
    "        z = self.merge_z(z)\n",
    "        \n",
    "        y = x.new(batch_size, 1).zero_() + 2 # index of <BOS>\n",
    "        \n",
    "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
    "        \n",
    "        h_t_tilde, y_hats, indice = None, [], []\n",
    "        decoder_hidden = z\n",
    "        \n",
    "        \n",
    "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
    "            emb_t = self.decoder_emb(y)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, h_t_tilde, decoder_hidden)\n",
    "            context_vector = self.attention(decoder_output, encoder_hidden, mask)\n",
    "\n",
    "            h_t_tilde = torch.cat([decoder_output, context_vector], dim=-1)\n",
    "            h_t_tilde = self.concat(h_t_tilde)\n",
    "            h_t_tilde = self.tanh(h_t_tilde)\n",
    "            \n",
    "            y_hat = self.generator(h_t_tilde)\n",
    "            \n",
    "            y_hats += [y_hat]\n",
    "            \n",
    "            if is_greedy:\n",
    "                y = y_hat.argmax(dim=-1)\n",
    "                \n",
    "            else:\n",
    "                # take random sampling\n",
    "                y = torch.multinomial(y_hat.exp().view(batch_size, -1), 1)\n",
    "                \n",
    "            # 이번 step에 EOS가 없을 경우 PAD\n",
    "            y = y.masked_fill_(~is_decoding, 0) # index of <PAD>\n",
    "            # 있을 경우 EOS\n",
    "            is_decoding = is_decoding * torch.ne(y, 3) # index of <EOS>\n",
    "            \n",
    "            indice += [y]\n",
    "            \n",
    "        y_hats = torch.cat(y_hats, dim=1)\n",
    "        indice = torch.cat(indice, dim=1)\n",
    "        \n",
    "        return y_hats, indice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7663a6",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4492459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModule(pl.LightningModule):\n",
    "    def __init__(self, model, output_size):\n",
    "        \n",
    "        super(CustomModule, self).__init__()\n",
    "        \n",
    "        \n",
    "        def get_crit(output_size, pad_index=1):\n",
    "            loss_weight = torch.ones(output_size)\n",
    "            loss_weight[pad_index] = 0.\n",
    "            crit = nn.NLLLoss(\n",
    "                weight=loss_weight,\n",
    "                reduction='sum'\n",
    "            )\n",
    "\n",
    "            return crit\n",
    "        \n",
    "        self.model = model\n",
    "        self.crit = get_crit(output_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "    \n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        return self.model(src, tgt)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        mini_batch, _ = batch\n",
    "        x, y = mini_batch[0], mini_batch[1][:, 1:]\n",
    "        y_hat = self(x, mini_batch[1][:, :-1])\n",
    "        loss = self.crit(y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "                         y.contiguous().view(-1))\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        mini_batch, _ = batch\n",
    "        x, y = mini_batch[0], mini_batch[1][:, 1:]\n",
    "        y_hat = self(x, mini_batch[1][:, :-1])\n",
    "        loss = self.crit(y_hat.contiguous().view(-1, y_hat.size(-1)),\n",
    "                         y.contiguous().view(-1))\n",
    "        metrics = {'val_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return  self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac7ca92",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96c7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa0b311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader:\n",
    "    def __init__(self, batch_size=64, max_length=70, shuffle=True, train=True):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.SRC = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            preprocessing=lambda x : x if len(x) < max_length else x[:max_length],\n",
    "            include_lengths=True,\n",
    "        )\n",
    "        self.TGT = data.Field(\n",
    "            sequential=True,\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            preprocessing=lambda x : x if len(x) < max_length else x[:max_length],\n",
    "            init_token='<BOS>',\n",
    "            eos_token='<EOS>'\n",
    "        )\n",
    "        \n",
    "        if train :\n",
    "            train, valid = data.TabularDataset.splits(\n",
    "                path='./kor_eng_translation/',\n",
    "                train='train.tsv',\n",
    "                validation='valid.tsv',\n",
    "                format='tsv',\n",
    "                fields=[('src',self.SRC), ('tgt', self.TGT)]\n",
    "            )\n",
    "\n",
    "            self.train_loader = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size,\n",
    "                device='cuda:0',\n",
    "                shuffle=shuffle,\n",
    "                sort_key = lambda x : len(x.tgt) + (80 * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "            self.valid_loader = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size,\n",
    "                device='cuda:0',\n",
    "                sort_key = lambda x : len(x.tgt) + (80 * len(x.src)),\n",
    "                sort_within_batch=True,\n",
    "            )\n",
    "            \n",
    "            self.SRC.build_vocab(train, max_size=30000, min_freq=5)\n",
    "            self.TGT.build_vocab(train, max_size=30000, min_freq=5)\n",
    "            \n",
    "    def load_vocab(self, src_vocab, tgt_vocab):\n",
    "        self.SRC.vocab = src_vocab\n",
    "        self.TGT.vocab = tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a3164b",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "230a31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dm = CustomDataLoader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faac41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2afd10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(dm.SRC.vocab)\n",
    "output_size = len(dm.TGT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3948f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30002, 30004)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2559d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(input_size, 512, 512, output_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d9b22a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "module = CustomModule(model, output_size)\n",
    "trainer = pl.Trainer(max_epochs=1, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "728821bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | Seq2Seq | 58.7 M\n",
      "1 | crit  | NLLLoss | 0     \n",
      "----------------------------------\n",
      "58.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.7 M    Total params\n",
      "234.892   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1dd8369ae740aa9020233161859b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcwee/anaconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 67. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(module, dm.train_loader, dm.valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d309979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'model': model.state_dict(),\n",
    "        'opt': module.optimizer.state_dict(),\n",
    "        'src_vocab': dm.SRC.vocab,\n",
    "        'tgt_vocab': dm.TGT.vocab,\n",
    "    }, './model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cacec4",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77bdf3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder_emb): Embedding(30002, 512)\n",
       "  (decoder_emb): Embedding(30004, 512)\n",
       "  (encoder): Encoder(\n",
       "    (rnn): GRU(512, 256, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (attention): Attention(\n",
       "    (linear): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): GRU(1024, 512, num_layers=4, batch_first=True, dropout=0.2)\n",
       "  )\n",
       "  (concat): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (generator): Generator(\n",
       "    (output): Linear(in_features=512, out_features=30004, bias=True)\n",
       "    (softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_data = torch.load(\n",
    "    './model.pth',\n",
    "    map_location='cuda:0' if torch.cuda.is_available else 'cpu'\n",
    ")\n",
    "loader = CustomDataLoader(train=False)\n",
    "loader.load_vocab(saved_data['src_vocab'], saved_data['tgt_vocab'])\n",
    "\n",
    "input_size = len(loader.SRC.vocab)\n",
    "output_size = len(loader.TGT.vocab)\n",
    "\n",
    "model2 = Seq2Seq(input_size, 512, 512, output_size).cuda()\n",
    "model2.load_state_dict(saved_data['model'])\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57980f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6485d9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 내일 학교에 가야 한다.\n",
      "[\"i ' m going to go to school .\"]\n"
     ]
    }
   ],
   "source": [
    "def to_text(indice, vocab):\n",
    "    lines = []\n",
    "    \n",
    "    for i in range(len(indice)):\n",
    "        line = []\n",
    "        for j in range(len(indice[i])):\n",
    "            index = indice[i][j]\n",
    "            \n",
    "            if index == 3:\n",
    "                break\n",
    "                \n",
    "            else :\n",
    "                line += [vocab.itos[index]]\n",
    "        line = ' '.join(line)\n",
    "        lines += [line]\n",
    "        \n",
    "    return lines\n",
    "        \n",
    "with torch.no_grad():\n",
    "    sentence = [input()]\n",
    "    \n",
    "    x = dm.SRC.numericalize(\n",
    "        ([tokenizer.morphs(sentence[0])], [len(tokenizer.morphs(sentence[0]))]),\n",
    "        device='cuda:0' if torch.cuda.is_available else 'cpu'\n",
    "    )\n",
    "    \n",
    "    y_hats, indice = model2.search(x)\n",
    "    output = to_text(indice, loader.TGT.vocab)\n",
    "    \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5611295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
